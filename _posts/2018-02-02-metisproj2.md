---
layout: post
title: Metis Project 2 - Web Scraping and Linear Regression
---

The second project has two major components: web scraping (BeautifulSoup, Selenium) and linear regression (ScikitLearn, StatsModels). I picked a topic that was on my mind since my previous job as a geomorphologist and costal engineerer: is it possible to predict the sediment concentration in the lower Mississippi River using upstream hydrological data?

### Motivation: save the sinking coasts with sediments
When talking about rivers, most people see them as passages of water. The Mississippi River drains XXX square miles. It is the biggest river in the United States and the XXth in the world. Same as many modern rivers, it is leveed to protect people from flooding, but that also cuts off the connection between the river and the surrounding floodplain. Think about the Nile River Delta: the rich soil brought by annual flooding nurtured one of the most ancient civilization. 

### Webscraping from USGS water data
**BeautfifulSoup and Selenium:** Hydrological data are available from hundreds of United States Geological Survey stations [USGS water data](https://waterdata.usgs.gov/nwis). I followed these steps to get the stations I needed for this project:

* Use keyword search to get a list of the stations on the tributary of interest
* For each station in the lists, follow the URL to return the data inventory information, from there determine which station has the type and range of data
* Download data file for those stations selected

<p align="center'>
  <img src="../../Liang_Metis/Project_2/imgs/Luther_usgspage.png">
</p>

**Data cleaning and transformation:** 

<p align="center'>
  <img src="../../Liang_Metis/Project_2/imgs/Luther_logtrans.png">
</p>

### Basic OLS
As a first attempt, an Ordinary Least Square (OLS) regression was fitted using all 113 stations as features. 

**Diagnose statistics:** the basic OLS model gives an R2 of 0.79 and a probability of F-statistics below 0.005 - it means the relationship is quite significant. Look further into the residuals, Omnibus and JB test are in the safe zone (p>0.05), the normal distirbution hypothesis could be kept. Although Durbin-Waston test shows autocorrelation in the residuals (<1.5) - not uncommon for time series.

<p align="center'>
  <img src="../../Liang_Metis/Project_2/imgs/Luther_basicfit.png">
</p>
<p align="center'>
  <img src="../../Liang_Metis/Project_2/imgs/Luther_basicstats.png">
</p>

**Multicollinearity:** many of the stations are not far from each other on the same tributary, so the multicollinearity between feautures are quite significant, i.e., the discharge/stage data are highly correlated. The heat map below visualize the situation:

<p align="center'>
  <img src="../../Liang_Metis/Project_2/imgs/Luther_multicoll.png">
</p>

**Cross-Validation:** here things got interesting. A 6-fold cross-validation was applied to the basic OLS model. It turned out that whether the rows are shuffled or not made a big impact. Remember that all data are time series, therefore splitting unshuffled data is equivalently training on a 5-year timeseries, and test on the 1-year that is left out. In the case of shuffled data, it essentially allows training set to randomly sample through the whole 6-year duration, and the model is able to learn from any year.

**Table: R-squared from 6-fold CV**
Processing | mean | std | fold-1 | fold-2 | fold-3 | fold-4 | fold-5 | fold-6
--- | --- | --- | --- | --- | --- | --- | --- | ---
**shuffled** | 0.75 | 0.026 | 0.75 | 0.70 | 0.77 | 0.78 | 0.74 | 0.77
**not-shuffled**| -0.44 | 0.50 | 0.27 | -0.37 |  0.29 | -0.92 | -0.17 | -1.73


  
### Regularization
To generalize the basic model and to improve out-of-sample performance, regularization is required to remove multicollinearity and excess features. 

**Lasso:** For the project presentation I chose Lasso Regression for its ability to quickly reduce the number of features. it gives a L1-norm penalty on coefficients and is good at eliminating features. I combined Lasso and 6-fold CV and tested 100 values between 1e-8 and 1.0 that are evenly spaced in log space. Again, shuffling the data made a big difference, while shuffled CV score remained higher and dropped with bigger penalty, unshuffled CV remained low but had a small peak with $\alpha ~= 0.05$ and a R-squared of 0.26.

<p align="center'>
  <img src="../../Liang_Metis/Project_2/imgs/Luther_basicLassoCV.png">
</p>

**Ridge:** After the project presentation I also experimented with Ridge, for its ability to reduce multicollinearity. 

**ElasticNet** Out of curiosity, the same process with ElasticNet is shown below:

### Recursive feature selection
Sklearn has a built-in function for feature selection [RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html). By specifying the number of features to keep, the function return the best fit by recursively testing a subset of the features of the given size. The scoring method is 6-fold cross-validation with shuffled data. Note the plunge of R-squared score around 15 features.

<p align="center'>
  <img src="../../Liang_Metis/Project_2/imgs/Luther_RFEscore.png">
</p>

A grid search is implemented by RFE (# of features from 0-15) and Lasso CV (alpha from 1e-2 to 1e-1, 9 values total). Best 

### Conclusion and future directions
* Using linear regression to predict downstream sediment concentration can explain more than 70% of the variance

### Final Thoughts

[RETURN TO BLOG](../)

---
layout: post
title: TailNet - Deep Learning for Marine Mammal Conservation 
---

This is my passion project for a reason. I love the ocean. Many of the most magical moments in my life have been the underwater encounters with marine mammals, such as the wild dolphins in these photos. It's still on my bucket list to swim with a whale one day.

When I saw there's a Kaggle competition on identifying humpback whales from the photos of their tails, I knew immediately what my final project would be. Humpback whale are gentle giants that grow up to 50 ft long. The unique black and white patterns on their tails make it possible to distinguish individuals. They used to be on the edge of extinction but are recovering slowly thanks to ocean conservation efforts. By joining this competition, we contribute to the scientific research on their population dynamics.

### The challenge

The biggest challenge comes from imply the lack of good data. There are more than 4000+ whale individuals, but only 9800 labeled photos to learn from. Most individuals have only one photo on record, and even this single shot varies greatly in image quality. Imagine how difficult it is to go through another 15000 unlabeled photos and figure out "have I seen you before? If so, which one?".

I see this problem as a pattern match problem in computer vision and decided to use convolutional neural networks as the fundation. ConvNets have been growing fast in recent years and proven to mimic how human vision works, that instead of seeing just pixels, it tries to understand the symantic structures in images.

I have also identified this specific challenge as a "one-shot" learning problem. I know it sounds very unfamiliar, but I bet many of you have heard of, of even experienced it yourself, the fancy FaceID on the new iphoneX? In general, face verification, telling whether you are actually you, is the building block for the more complex problem of face recognition, finding out who you are. When Apple was developing this model, very likely a neural network but of course it's not open source, they could not have trained the model on the information of your face, or any consumer's face, but everyone's phone is able to identify you as you after just one initial setup. How does that work?

Here's how I would construct a faceID from scratch, but let's use the whale phots so you know I'm not going off track. After initial processing, pairs of images are fed through two identical ConvNets and become a pair of vectors at the end of the pipeline. This is called an "embedding" or "encoding" process, which concentrate the dimensions down to the most meaningful features. From there, by looking at the distance between the two feature vectors, we decide whether it's the same whale or not. The key is to train the network so that the features separate different individuals apart. With this blueprint, let's go to the assembly line.

In TailNet 1.0, I took a basic approach for each part. All training photos are processed and cropped using OpenCV, hoping to increase the signal to noise ratio. 

To obtain the feature vectors, I first experimented with a few pre-trained convnets with frozen weights based on ImageNet. The idea is that these convnets are trained to predict from 1000 categories of a diverse set of objects. The hope is that if discard the prediction layer for the 1000 labels, the feature vector underneath should overall capture a great deal of the characteristics of the patterns in the input images. Classification is done using a simple nearest neighbor method.

With TailNet 1.0, I'm currently at 12th place out of 139 teams, after joining the competition only recently while it has been running for 3 months. The performance has a lot of room to improve. 

Here's TailNet 2.0 that is running right now on AWS. This upgraded version is a conjoined-twins network, know as a Siamese Neural Network. For the identical twin convnets, I selected the best performer from TailNet 1.0, the InceptionV3, and made the last 5% weights trainable, hoping to get more meaningful tail features in the output vectors. The loss function is designed to minimize distance between same whale and the opposite for different whales. To overcome lack of data, I applied real-time image augmentation in Keras to simulate different conditions. I hope to submit another result to Kaggle soon.

In summary, TailNet is an example of face recognition applied to a different domain. The basic approaches already got me to a good start and please stay tuned as there are still 3 month till the competition ends. I envision TailNet and similar networks can be applied to a wide range of wildlife identifiation and monitoring, and beyond.